{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# LangGraph 体验\n",
    "\n",
    "## Chapter 3: LangGraph with Human\n",
    "\n",
    "Human in the loop 是指在 Agent 的工作流中，插入人类的参与。这样的设计将便于更好地控制工作流的走向，迎着用户更加满意的方向。\n",
    "\n",
    "其实前文改变、更新、操纵工作流的过程已经充分体现了 Human in the loop 的设计思想，都是借助了外部的因素来控制工作流的走向。但为了更加方便的实现用户在 Agent 决策过程中的作用，我们还能够让人类能够更加有选择性地参与到工作流中。\n",
    "\n",
    "正如我们一直所说的，State 类是整个工作流中的关键变量，我们可以通过定制化的 State 类来更好实现 Human in the loop 的设计。"
   ],
   "id": "8528e4b7e3baeac7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "LLM_NAME = os.getenv(\"ZHIPU_LLM_NAME\") or None\n",
    "API_BASE = os.getenv(\"ZHIPU_API_BASE\") or None\n",
    "API_KEY = os.getenv(\"ZHIPU_API_KEY\") or None"
   ],
   "id": "c9cd4bb174ca12f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. 自定义 State 类 —— State 与个工作流节点的有机结合\n",
    "\n",
    "State 类对于整个有向图的构建是至关重要的，下文我们将构建一个含人的调用状态的 State 类，以便更好地实现 Human in the loop 的设计。\n",
    "\n",
    "假定当前有一个业务，用户问了一个非常非常专业的问题，但是 LLM 发现自己不太能回答，则此时需要另一个专家来帮忙回答。则此时被找的专家即为我们要实现的 Human in the loop 中的人类！"
   ],
   "id": "c2ef77c3b9e2b65d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    # This flag is new\n",
    "    ask_human: bool"
   ],
   "id": "fd0fb22ea2dbb902",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "随后，我们定义一个 Pydantic 类，用于定义一个请求专家帮助的请求。\n",
    "\n",
    "官方文档提示我们要用 pydantic v2 而不是 v1"
   ],
   "id": "e0c01621d62c4481"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class RequestAssistance(BaseModel):\n",
    "    \"\"\"Escalate the conversation to an expert. Use this if you are unable to assist directly or if the user requires support beyond your permissions.\n",
    "\n",
    "    To use this function, relay the user's 'request' so the expert can provide the right guidance.\n",
    "    \"\"\"\n",
    "\n",
    "    request: str"
   ],
   "id": "fa0fa6fae79f3935",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "再次定义一个 chatbot 节点，但这次在两处有改动。\n",
    "\n",
    "- 一是多绑定了一个工具，RequestAssistance，用于接受用户的请求并将其转发给专家。在 LangChain 中，工具的定义可以是一个 Pydantic 类，也可以是一个继承了 BaseTool 的类。\n",
    "- 二是在 chatbot 函数中，当 LLM 认为自己不能回答的时候，会将 ask_human 设为 True，更新 state，以便后续的工作流能够更好地处理这种情况。"
   ],
   "id": "e5a2aae2b9c2ec3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "wrapper = DuckDuckGoSearchAPIWrapper(max_results=5)\n",
    "tool = DuckDuckGoSearchRun(api_wrapper=wrapper)\n",
    "tools = [tool]\n",
    "llm = ChatOpenAI(model=LLM_NAME, openai_api_key=API_KEY, openai_api_base=API_BASE)\n",
    "llm_with_tools = llm.bind_tools(tools + [RequestAssistance])\n",
    "\n",
    "def chatbot(state: State):\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    ask_human = False\n",
    "    if (\n",
    "        response.tool_calls\n",
    "        and response.tool_calls[0][\"name\"] == RequestAssistance.__name__\n",
    "    ):\n",
    "        ask_human = True\n",
    "    return {\"messages\": [response], \"ask_human\": ask_human}"
   ],
   "id": "6483803b045e9619",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "随后就是很普通的构建工具、构建图的过程了。",
   "id": "13ede349cb911850"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_node(\"tools\", ToolNode(tools=[tool]))"
   ],
   "id": "76eee15c8eda4ee4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "\n",
    "# 此处定义了一个工具函数，用于根据 AI 消息的工具请求消息，构建一个工具回答消息\n",
    "def create_response(response: str, ai_message: AIMessage):\n",
    "    return ToolMessage(\n",
    "        content=response,\n",
    "        tool_call_id=ai_message.tool_calls[0][\"id\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def human_node(state: State):\n",
    "    new_messages = []\n",
    "    if not isinstance(state[\"messages\"][-1], ToolMessage):\n",
    "        # Typically, the user will have updated the state during the interrupt.\n",
    "        # If they choose not to, we will include a placeholder ToolMessage to\n",
    "        # let the LLM continue.\n",
    "        new_messages.append(\n",
    "            create_response(\"No response from human.\", state[\"messages\"][-1])\n",
    "        )\n",
    "    return {\n",
    "        # Append the new messages\n",
    "        \"messages\": new_messages,\n",
    "        # Unset the flag\n",
    "        \"ask_human\": False,\n",
    "    }\n",
    "\n",
    "\n",
    "graph_builder.add_node(\"human\", human_node)"
   ],
   "id": "2fe048e3821a32c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "前文一直反反复复的调整反转 state 对象中 `ask_human` 的值，目的就是为了控制他是否能够选择 human 工具节点。有一种思想是在工具内部，查看 `state.ask_human` 的值，但是更为精准的做法则是使用条件边。\n",
    "\n",
    "条件边的用法可见 ch1。\n",
    "\n",
    "我们将上文定义的内容都组装成一个完整的有向图，然后使用 MemorySaver 来保存中间状态。如下文。"
   ],
   "id": "c9122ef31fed5c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.constants import START\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "\n",
    "def select_next_node(state: State):\n",
    "    if state[\"ask_human\"]:\n",
    "        return \"human\"\n",
    "    # Otherwise, we can route as before\n",
    "    return tools_condition(state)\n",
    "\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    select_next_node,\n",
    "    {\"human\": \"human\", \"tools\": \"tools\", \"__end__\": \"__end__\"},\n",
    ")\n",
    "\n",
    "# The rest is the same\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(\"human\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(\n",
    "    checkpointer=memory,\n",
    "    # We interrupt before 'human' here instead.\n",
    "    interrupt_before=[\"human\"],\n",
    ")"
   ],
   "id": "bc238490bd59f0d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "让我们尝试问一个专业问题，引导 LLM 选取 human 节点，看看会发生什么吧！",
   "id": "69ad6df81b406b03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "user_input = \"I need some expert guidance for building this AI agent. Could you request assistance for me?\"\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "# The config is the **second positional argument** to stream() or invoke()!\n",
    "events = graph.stream(\n",
    "    {\"messages\": [(\"user\", user_input)]}, config, stream_mode=\"values\"\n",
    ")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ],
   "id": "b024f9c1119f6959",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "snapshot = graph.get_state(config)\n",
    "snapshot.next"
   ],
   "id": "95b7f8972131ae63",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "通过查看用户问询结束后的 graph 状态，我们可以看到下一个节点是 human 节点。\n",
    "\n",
    "如果我们就这样推进下去，graph 就会进入 human 节点了，但是这个程序在设计 human 节点时，默认用户在 human 节点中获取不到任何信息，则我们此时可以通过人为添加对话来模拟 human 的回答。"
   ],
   "id": "d8b24cc8d72aafe9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ai_message = snapshot.values[\"messages\"][-1]\n",
    "human_response = (\n",
    "    \"We, the experts are here to help! We'd recommend you check out LangGraph to build your agent.\"\n",
    "    \" It's much more reliable and extensible than simple autonomous agents.\"\n",
    ")\n",
    "tool_message = create_response(human_response, ai_message)\n",
    "graph.update_state(config, {\"messages\": [tool_message]})\n",
    "for message_pieces in graph.get_state(config).values[\"messages\"]:\n",
    "    message_pieces.pretty_print()"
   ],
   "id": "15b08ab222c4c041",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "在上面的案例中，我们又伪造了一个来自 human 的回答，最后再交给 LLM 看看他会怎么处理，最后我们再回顾一下这次对话的全部内容吧！\n",
    "\n",
    "复习一下，再次调用 stream() 方法，第一个参数给 None，将会从断点处继续推进 graph 的状态。"
   ],
   "id": "21d34db427db69dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "events = graph.stream(None, config, stream_mode=\"values\")\n",
    "for message_pieces in graph.get_state(config).values[\"messages\"]:\n",
    "    message_pieces.pretty_print()"
   ],
   "id": "7df46b6d8ddfa6e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. 倒带 （做不出😭😭😭）\n",
    "\n",
    "若在某些对话过程当中，用户想要修改其中某句话的表述，想要回到某个特定的 LLM 上下文状态怎么办？\n",
    "\n",
    "这种情况下就要用到 LangGraph 中 graph 对象的 `get_state_history` 了！\n",
    "\n",
    "承接上文，让我们来构造一次普通的问答请求。为了避免前文消息的干扰，我们重新构造一个两轮对话，令 `thread_id` 为 3。"
   ],
   "id": "4830d625801b8221"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            (\"user\", \"I'm learning LangGraph. Could you do some web research on it for me?\")\n",
    "        ]\n",
    "    },\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "\n",
    "events = graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            (\"user\", \"Ya that's helpful. Maybe I'll build an autonomous agent with it!\")\n",
    "        ]\n",
    "    },\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ],
   "id": "4b2875c4ce76ce7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "现在再写一个代码，来查看 graph 的状态历史吧。",
   "id": "1d15d174d669dc35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "to_replay = None\n",
    "for state in graph.get_state_history(config):\n",
    "    print(\"Num Messages: \", len(state.values[\"messages\"]), \"Next: \", state.next)\n",
    "    print(\"-\" * 80)\n",
    "    if len(state.values[\"messages\"]) == 6:\n",
    "        # We are somewhat arbitrarily selecting a specific state based on the number of chat messages in the state.\n",
    "        to_replay = state   # 这个 to_replay 定为 6，即第二轮对话结束后的状态——最新状态，有什么用？？"
   ],
   "id": "d3b65cffa31c826b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "通过上面的代码，我们获取到了一个目标的状态点，打印一下看看？",
   "id": "cf30829957c88a59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(to_replay.next)\n",
    "print(to_replay.config)"
   ],
   "id": "f2205d5baa4573b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "我们发现，我们刚拿到的这个状态点下一步没事干（`to_replay.next` 内容为空），而且 config 也没什么特别的。但这并不重要，重点是我们可以根据这样一个检查点，重新推进对话。",
   "id": "6a70cf77a4e24627"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# The `checkpoint_id` in the `to_replay.config` corresponds to a state we've persisted to our checkpointer.\n",
    "for event in graph.stream(None, to_replay.config, stream_mode=\"values\"):\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ],
   "id": "393c0c7814f3dc29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "难道这个检查点的意义就局限于此？就这么无聊一个应用？\n",
    "\n",
    "下文将介绍如何更新其中的某一个步骤，并且重新从那一个步骤开始推进对话。\n",
    "\n",
    "（好烦，这个倒带做不出，不知道哪里出问题了😭😭😭）"
   ],
   "id": "b62c07a0f167f9e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "to_replay_again = None\n",
    "for state in graph.get_state_history(config):\n",
    "    print(\"Num Messages: \", len(state.values[\"messages\"]), \"Next: \", state.next)\n",
    "    print(\"-\" * 80)\n",
    "    # 这样写 if 是因为到 START 节点跟到上一轮的 END 节点的 messages 长度是一样的（都是4，可以观察之前的 state 陈列结果）。要倒带肯定是倒到有实际 message 的地方（即 START 处），所以这里的 if 语句是为了找到 START 节点，怕跟上一轮的 END 节点混淆。\n",
    "    # 总结：为了找到 START 节点，规避掉上一轮的 END 节点。\n",
    "    if len(state.values[\"messages\"]) == 4:  \n",
    "        # We are somewhat arbitrarily selecting a specific state based on the number of chat messages in the state.\n",
    "        to_replay_again = state\n",
    "\n",
    "print(\"\\n\\n replay node: \")\n",
    "print(to_replay_again.metadata)\n",
    "print(to_replay_again.next)\n",
    "print(to_replay_again.config)\n",
    "\n",
    "for event in graph.stream(None, to_replay_again.config, stream_mode=\"values\"):\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ],
   "id": "4241f1c0d19c2530",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "last_message = to_replay_again.values[\"messages\"][-1]\n",
    "\n",
    "# 修改第二轮对话的用户输入\n",
    "last_message.content = \"Thanks you! I will take your information and build a chatbot with LangGraph! Before that, I also wanted to know about LangChain, could you help me with that?\"\n",
    "\n",
    "branch_config = graph.update_state(\n",
    "    to_replay_again.config,\n",
    "    {\"messages\": [last_message]},\n",
    ")\n",
    "print(branch_config)"
   ],
   "id": "e9e46c1036962201",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "graph.stream(None, branch_config, stream_mode=\"values\")",
   "id": "ff1fb220dd22447c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for message_pieces in graph.get_state(config).values[\"messages\"]:\n",
    "    message_pieces.pretty_print()"
   ],
   "id": "c558cb35eda60267",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
